{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\EXTRACT\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import fitz \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from htmlTemplates import css, bot_template, user_template\n",
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text (PDF --> Image -OCR-> TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_text(pdf_docs, zoom_x=2.0, zoom_y=2.0):\n",
    "    \n",
    "    text = ''\n",
    "    for pdf_path in pdf_docs:\n",
    "        pdf_file = fitz.open(pdf_path)\n",
    "        for page_num in range(len(pdf_file)):\n",
    "            page = pdf_file[page_num]\n",
    "            matrix = fitz.Matrix(zoom_x, zoom_y)  # Zoom factor for higher resolution\n",
    "            pix = page.get_pixmap(matrix=matrix)\n",
    "\n",
    "            # Convert image to PIL format\n",
    "            image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "            \n",
    "            # Convert PIL image to OpenCV format\n",
    "            img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Preprocess the image if necessary\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            _, thresh = cv2.threshold(gray, 210, 230, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Perform OCR using Tesseract\n",
    "            ocr_text = pytesseract.image_to_string(thresh)\n",
    "            \n",
    "            text += ocr_text\n",
    "        pdf_file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['data/sample1.pdf']\n",
    "text = convert_pdf_to_text(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musterfirma — MusterstraBe 23 - 12345, Musterhausen\n",
      "Gutschriftsempfanger\n",
      "\n",
      "StraSe, Hausnummer\n",
      "\n",
      "PLZ, Ort\n",
      "\n",
      "Gutschrift\n",
      "\n",
      "Sehr geehrter Herr Schmidt,\n",
      "\n",
      "[thr Logo]\n",
      "\n",
      "Musterfirma AG\n",
      "Musterstrake, 23\n",
      "12345, Musterhausen\n",
      "\n",
      "Datum: 01.03.2019\n",
      "Gutschrift Nr.: 2019-1004\n",
      "\n",
      "Ihre UmsSt. ID: DE123456789\n",
      "Gutschriftssdatum entspricht Liefer-/Leistungsdatum\n",
      "\n",
      "gema® unserer Vereinbarung schreiben wir Ihnen folgende Leistungen gut:\n",
      "\n",
      "Position Anzahl Einheit Bezeichnung\n",
      "\n",
      "1 5 Stick  Musterleistung\n",
      "2 3 Stick Musterleistung\n",
      "Nettopreis\n",
      "\n",
      "2zgl. 19% USt.\n",
      "\n",
      "Gutschriftbetrag\n",
      "\n",
      "Einzelpreis Gesamtpreis\n",
      "\n",
      "3,00€ 15,00 €\n",
      "5,00 € 15,00 €\n",
      "30,00 €\n",
      "\n",
      "5,70€\n",
      "\n",
      "35,70 €\n",
      "\n",
      "Wir dberweisen Ihnen den Gutschriftbetrag in den nachsten Tagen auf Ihr Konto.\n",
      "\n",
      "Mit freundlichen GrifBen\n",
      "Max Mustermann\n",
      "\n",
      "Musterfirma GmbH KreditInstitut: Commerzbank USt-ID: DE24324567\n",
      "\n",
      "MusterstraBe, 23 IBAN: DE3423 4562 3435 6765 HI\n",
      "12345, Musterhausen BIC: COBADEFFXXX\n",
      "Kto. Inh.: Max Mustermann\n",
      "\n",
      "Tel: +40 (0)30 12345678\n",
      "E-Mail: Info@muster.de\n",
      "\n",
      "12345678\n",
      "Amtsgericht: Charlottenburg\n",
      "Geschaftsfidhrer: Max Mustermann\n",
      "Webseite: www.firma.de\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     # Remove extra whitespace and special characters\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "#     text = re.sub(r'[^\\w\\s.,;:!?]', '', text)\n",
    "#     text = text.strip()\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_chunks(text):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = get_text_chunks(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\EXTRACT\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\HP\\anaconda3\\envs\\EXTRACT\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "# model_name = 'intfloat/multilingual-e5-large-instruct'\n",
    "model_kwargs = {\n",
    "    \"device\": \"cpu\", \"trust_remote_code\": True\n",
    "    }\n",
    "encode_kwargs = {\n",
    "    \"normalize_embeddings\": False,\n",
    "}\n",
    "embeddings= HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorstore(text_chunks,embeddings):\n",
    "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = get_vectorstore(text,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_chain(vectorstore):\n",
    "    \n",
    "    llm = HuggingFaceHub(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
    "\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\EXTRACT\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "conversation_chain = get_conversation_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "]\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: name the items in the invoice\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "Question: name the items in the invoice\n",
      "Helpful Answer: The items in the invoice are: 1. Fruits (Apples, Bananas, Oranges) 2. Vegetables (Carrots, Potatoes, Tomatoes) 3. Meat (Chicken, Beef) 4. Dairy Products (Milk, Eggs) 5. Bread (Loaf of Bread) 6. Rice (Bag of Rice) 7. Sugar (Bag of Sugar) 8. Tea (Pack of Tea) 9. Coffee (Pack\n",
      "Follow Up Input: translate\n",
      "Standalone question: _______________________________________________________\n",
      "\n",
      "Translation of the follow-up input is not necessary. The follow-up input is simply a request to rephrase the follow-up question into a standalone question. Since the original question was \"translate\", the rephrased standalone question is:\n",
      "\n",
      "What do you want me to translate?\n",
      "Helpful Answer: What do you want me to translate? (The original question was already in English, so there is no need to translate anything.)\n"
     ]
    }
   ],
   "source": [
    "# Pass a question to the conversation chain\n",
    "question = \"translate\"\n",
    "response = conversation_chain.run(question)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Extract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
